\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1.3in]{geometry}
\usepackage[hidelinks]{hyperref}
%\usepackage{eurosym}
\usepackage{caption}
\usepackage{titling}
\usepackage{xcolor,colortbl}
\usepackage{tabularx}
%------------------------------------------
\captionsetup[table]{position=bottom}
\setlength{\parindent}{0pt}
\definecolor{Gray}{rgb}{0.7,0.7,0.7}
\pretitle{\begin{center}\Large\bfseries}
%------------------------------------------



\title{
  Serverless technologies comparison\\
  \large Large Systems project}
\author{Sean Liao and Mar Badias}



\begin{document}

\maketitle

%% TODO abstract
%% TODO introduction
  %% -> why our reaserch is relevant, what are this services exactly
  %% -> why can we put Caas and FaaS in the same bag

%% TODO related work
%% TODO research question
%% - business case
%% TODO methodology
%% - variables
%% - CPU
%% - overhead / latencies
%% - pricing model
%% - test setup
%% TODO results
%% - data
%% - pricing
%% TODO discussion
%% - applicability
%% - billing
%% - other tech
%% - answer reaserch question
%% TODO usage case
%% TODO conclusion
%% TODO references


\begin{abstract}
Write abstrat here
\end{abstract}



\section{Introduction}
\label{introduction}
Public clouds are growing, and with it comes the latest push into serverless offerings. These come in many forms depending on the abstraction level, but they can largely be grouped into: long-lived containers, short-lived containers, functions as a service. \\

Long-lived containers or Platform as a Service (PaaS). They represent the first generation of serverless technologies. These can be full-fledged, stateful applications, packaged in containers. The clouds will take these and run them for you on VMs. Auto-scaling and load balancing is usually offered, but fast startup times are not guaranteed. These should be considered an alternative UI to the underlying VMs, which will be reflected in the pricing model (charge for underlying VMs). Examples: AWS Elastic Container service, GCP App Engine, Azure App Services, Alibaba Container Service. \\

Functions as a Service (FaaS). Currently the highest level of abstraction they are the second generation of serverless technologies, developers provide their application code for the clouds to compile, package, deploy, and run. These are short-lived and stateless, an instance may be started for every request and killed after it completes. Billing is only for the time it is running serving a request. Examples: AWS Lambda, GCP Cloud Functions, Azure Functions, Alibaba Function Compute, IBM Cloud Functions, Zeit Now.\\

Short-lived containers or Containers as a Service (CaaS). Is the newest technologies short-lived, stateless runtimes and a similar billing model. Where they differ from FaaS is that they introduce containers, giving developers control of the execution environment, allowing them to run languages or runtimes unsupported by FaaS. Examples: AWS Fargate, GCP Cloud Run, Azure Container Instance, Alibaba Elastic Container Instance.


\section{Research question}

Given the amount of services of this type in the current market we defined a buisness case based on the most popular features \cite{popular1} of serverless computing: dynamically managed runtimes, autoscaling, globally reachable HTTP(S) endpoint and pay only for usage. Having defined our preferences, we want to look in their profiles and answer the following question:

\begin{itemize}
\item \textbf{When one should be chosen over the others when all are available?}
\end{itemize}

For deciding which is preferable we will consider the following subquestions:

\begin{itemize}
\item \textbf{Which has better raw computer performance?} With this question we will determine which solution provides the faster performance. While some platforms do provide numbers, we aim to check if these are comparable across services. Better here would be a faster completion of tasks, and/or a corresponding reduction in costs.

\item \textbf{Which one has lower platform overhead?}  We plan to measure the excess of computation time introduced by the platform as it supposes extra time in client request that we wish to minimize. Lower platform overhead will be considered more preferable.

\item \textbf{Which one has lower cold start latencies?} When a new instance receives its first request, the response time increases because this instance must be created. As answering the client request needs to be done as fast as possibles, lower cold start will be considered preferable.
\end{itemize}


% focus on single v multi
% and cold starts and burst workloads
% single = baseline



%We will not be testing long-lived containers as they require a different application architecture for effective utilization and should be compared to raw VMs.


\section{Methods}
\label{methods}
We want to test the products offered by the top 5 cloud providers as of 2019: Amazon Web Services (AWS), Google Cloud Platform (GCP), Microsoft Azure, Alibaba Cloud, and IBM Cloud. Additionally we will also test Zeit, a startup in the FaaS space popular for its streamlined experience. In the table \ref{Tab:services} we can see the products we will work with and in wich category they fit. \\

% Include in table: instance size (memory), python runtime, web framework

\begin{table}
\centering
 \begin{tabularx}{\textwidth}{ X  X  X }
 %\hline
 %\rowcolor{Gray}
 \textbf{Type} & \textbf{Platform} & \textbf{Product} \\
 \hline
 \hline
 FaaS & AWS & Lambda \\
 \hline
 FaaS & GCP & Functions \\
 \hline
 FaaS & Azure & Functions \\
 \hline
 FaaS & IBM Cloud & Functions \\
 \hline
 FaaS & Alibaba Cloud & Function Compute \\
 \hline
 FaaS & Zeit & Now \\
 \hline
 CaaS & GCP & Cloud Run \\
 \hline
 PaaS & GCP & App Engine \\
 %\hline

\end{tabularx}
\caption{Serverless services that will be tested in this reaserch.}
\label{Tab:services}
\end{table}

As mentioned in section \label{introduction}, the the pricing model for Pass, charge for underlying VMs, does not fit in the 'Pay only for usage' defined in our Reaserch question. However GCP App Engine offers the possiblitily to scale your applications to 0 when it is not been used so it fits into our buissnes case.



%%What we are going to measure

For answering the reaserch questions we will measure following variables:

\begin{itemize}
\item Overall performance. We want to obtaint how much does it take for each services to performe a request
\item Platform overhead.
\item Compute performance.
\end{itemize}


- we will get the ServerUID which is the instance UID
- time of the day we execute it


%How we are going to measure it.

-using images that their size are somehow ok. diferent formats but they follow a normal distribution, which makes their okok.

We want to use image processing as our test workload, a real world \cite{ii} use case. Specifically we will be testing image resizing (thumbnail creation). We aim to use the same code for all platforms (excluding API adapters) and our choice of language is Python because is one of the few languages that all platforms support.

Image processing was selected as it represents a common workload that, without hardware accelerators, relies heavily on CPU performance. It also does not require access to external resources, such as databases, which while also a common workload, introduce too much variability.

We will send images as HTTP requests to the various platforms to be resized. The code we deploy on the platforms will be responsible for both resizing and measuring the time it takes to do so. This will form the basis for our calculations of compute performance. We will additionally measure roundtrip time for requests from the client, this, minus the computation time will be used to calculate the platform overhead.  The same experiment will be performed for obtaining the cold start latencies but with different timing to ensure that the instance is created when receiving the request.








For each service, which config did we used and why

%------







Our plan is to spread out testing over a week, to even out variability from running at different times. Specifically we want to test hourly over a week for compute and overhead at single and 50 concurrent requests and repeat it 10 times, to cover the different concurrency guarantees of different products, and every 3 hours for cold start times (to allow time for the function to be evicted from local caches, additional testing required).

At the end of the experiments, the billing of each service will be compared too.






\section{Related work}
Cloud FaaS performance has been subject of previous research. An excellent example is \cite{aa} where multiple serverless providers are continuously been benchmarked. Their points of comparison will be used in this project for measuring and comparing them short-lived containers. Another example of an excellent comparison of Faas providers is \cite{bb}. Cloud short lived containers have also been a topic of study, comparing them to long live containers or performance test among of different provides. We can find examples of this in \cite{cc} and \cite{dd}. Comparison between Faas and short-lived containers has also been studied but from a functionality point of view, oversimplificating it and not taking into account the performance \cite{ee}\cite{ff}\cite{gg}.

Up to our knowledge, the performance differences between Faas and short live containers have not been comprehensively analysed yet, which motivates our research.
%SEAN WANTED TO ADD SOMETHING
READ THIS AND SEE WHAT CAN U INCLUDE HERE
\url{https://sci-hub.se/10.1002/cpe.4792}


\section{Results}
After performing the mentioned experiments we obtained the following results:

% Say: alibaba has a low succes rate so its results bla bla
% Say: other rare transient failures considered negligible
% Overall performence does not change during time (time of the day does not matter)

%presentation: does it work?
%talk about the x and y axis if necessary

\subsection{Overall performance}

The table shows. Parallel instances say they are parallel duirng a single test cycle.
%inset tambe out8 -> cols(single and multi): server, total requests, succes rate, cold sarts percentage (100-warm ratio),  parallel instances and standard desviation

% include perf over time

Overall performence does not change during time (time of the day does not matter)

\subsection{Overhead}

In figure XX we can find one plot for each service we have analized. They show the overhead of single vs muntiple and the other cold vs warm.

% Say: How this was measured: t1 - t2
% Say: What this includes: network, load balancing, scheduling, ...
% explain missing appengine line

%plotde les curves, baixdetot out17.


\subsection{Compute performance}

In figure XX we can see strictly the time of resizing the image in each test(multi vs single).



%plot only concurrency

\subsection{Price to performance}
%plot to draw.



\section{Discussion}

\subsection{Scaling}
From Table 2 % table number?
we observed elevated failure rates for Alibaba Function Compute.
Their documentation % https://www.alibabacloud.com/help/doc-detail/51907.htm
states they have a limit of 50 instances per service,
and even though our client places a strict limit of 50 inflight requests at any point in time,
their scheduler might not be keeping up.

Also from Table 2 % table number
we can see the average number of unique instances per test cycle.
Ideally, this number would be 50,
to match the number of parallel requests we are sending.

On the low end,
Azure Functions is notable for only starting 4 instances.
This may be from a documented limitation % https://docs.microsoft.com/en-us/azure/azure-functions/functions-scale
of only starting at most 1 instance per second with HTTP triggers.
Their load balancers still hold the requests in queue,
but they appear to be reluctant to start new instances even as sufficient time has passed.

At the high end,
we configured GCP Cloud Run to accept up to 5 concurrent requests per instance,
expecting it to utilise the available memory and CPU more efficiently.
Unfortunately we are unsure if this negatively impacted their scheduler
as we observed a less efficient reuse of instances
and greater variability in the number of instances started.

\subsection{Warm vs Cold Starts}
An often cited concern of using truly on-demand compute services is the cold start,
when the platforms have to start up a new instance to handle increases in load.
This is such a concern that some some platforms have specific features
built to keep your instances warm, such as AWS Provisioned Concurrency % https://aws.amazon.com/about-aws/whats-new/2019/12/aws-lambda-announces-provisioned-concurrency/
and Azure Functions Premium Plan.

Our results in Figure 2 % number?
show that, as expected, cold starts are slower than warm starts in most cases.


\subsection{Overall Performance}

\subsection{Overhead}
- cold is higher (as expected) because it has to iniciate the instance.


AWS landa load balancer is better because no differenec un single vs munlti
Multi is slower because load balancer can be a bottleneck.
if the graf increases at the end is bad because it means that the difference in overhead between request is high: the overherad is not stable. overhead is not stable in gpc-fun.
What might affect this overhead? ->
alibab at the end warm jumps


\subsection{Computer performance}
Something about CPU freq and how
green and purple line very togethrt-> provide a good isolation of resources
gpc-run results expected because it can handle 5 req at the same time.
IMB and alibaba -bad isolation.
if the lines grows, the platform gives(flat) or not consitency to the request (not very important here.)



\subsection{Scaling}

-alibaba succes rate is the lowest because it has to many request, do not scales well. Does it invalidate results?
gpc-fun there are outlayers.

- azure just creates 4 instances.
-aws is very consistent. for both overhead and cpu time/servertime2
- (of the table) -> say we are sending 50 paralel req. Some of them gives us 50 instances but gcp-run gives us many instances but the standard desviation is really high which it means its very insconsitency giving instances???
- if its lower then 50 means they are not doing it one to one. If there is more than 50, they are not good at reusing instances. gcp-run
-gcp-run may have been affected by us, when telling it that just 5 req per instance.
-GCP-APP-enginee it scales depending on cpu utilitzation and it looks like it consistencly handles 2 req per instance.

\subsection{Price to performance}


\subsection{Technologies}
how does this explain about performance.

\subsection{Research Questions}
(answer research questions)


\bibliography{references}
\bibliographystyle{unsrt}



\end{document}
