\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1.3in]{geometry}
\usepackage[hidelinks]{hyperref}
\usepackage{caption}
\usepackage{titling}
\usepackage{xcolor,colortbl}
\usepackage{tabularx}
\usepackage{changepage}
\usepackage{multicol}
\usepackage{graphicx}

%------------------------------------------

\graphicspath{ {./img/} }
\captionsetup[table]{position=bottom}
\setlength{\parindent}{0pt}
\definecolor{Gray}{rgb}{0.7,0.7,0.7}
\pretitle{\begin{center}\Large\bfseries}

%------------------------------------------

\title{
  Serverless technologies scaling comparison\\
  \large Large Systems project}
\author{Sean Liao and Mar Badias}


\begin{document}

\maketitle


\begin{abstract}
This research has tested and compared how different serverless services scale when there are spikes in traffic that increase the workload in a CPU bound environment. We have tested the flowing services: AWS Lambda, GCP Functions, Azure Functions, IBM Cloud Functions, Alibaba Cloud Function Compute, Zeit Now, GCP Cloud Run and GCP App Engine.

We have focused in comparing the performance of the client's application and the overhead introduced by the platform, testing cold starts (when a new instance of the application is created) and warm starts (reusing previously created instances). We concluded that scaling affects heavily the performance and platform overhead of almost all services excluding AWS Lambda, which presents a stable behaviour under all the performed tests.
\end{abstract}


\section{Introduction}
\label{introduction}

Public clouds are growing, and with it comes the latest push into serverless offerings. These come in many forms depending on the abstraction level, but they can largely be grouped into: long-lived containers, short-lived containers, functions as a service. \\

Long-lived containers or Platform as a Service (PaaS) represent the first generation of serverless technologies. These can be full-fledged, stateful applications, packaged in containers. The clouds will take these and run them for you on VMs. Auto-scaling and load balancing is usually offered, but fast startup times are not guaranteed. These should be considered an alternative UI to the underlying VMs, which will be reflected in the pricing model (charge for underlying VMs). Examples: AWS Elastic Container service, GCP App Engine, Azure App Services, Alibaba Container Service. \\

Functions as a Service (FaaS), currently the highest level of abstraction and the second generation of serverless technologies. Developers provide their application code for the clouds to compile, package, deploy and run. These are short-lived and stateless, an instance may be started for every request and killed after it completes. Billing is only for the time it is running serving a request. Examples: AWS Lambda, GCP Cloud Functions, Azure Functions, Alibaba Function Compute, IBM Cloud Functions, Zeit Now.\\

Short-lived containers or Containers as a Service (CaaS) is the newest technologies short-lived, stateless runtimes and a similar billing model. Where they differ from FaaS is that they introduce containers, giving developers control of the execution environment, allowing them to run languages or runtimes unsupported by FaaS. Examples: AWS Fargate, GCP Cloud Run, Azure Container Instance, Alibaba Elastic Container Instance. \\


With serverless architectures developers do not need to worry about managing, provisioning or purchasing servers. Moreover, their services offer more flexibility, quicker time to release and great scalability. \\


Our research will measure, analyse and compare their CPU performance, platform overhead and specially their capacity to scale when the workload increases unexpectedly. Scaling entails creating more instances, known as \textbf{cold starts}, and assigning new resources to them and it should be done without influencing others' instances performance.


\section{Related work}

Serverless technologies have been subject of previous research.
We can find good examples of PaaS analysis in \cite{googleappengine} and \cite{googleappengine2},
where the researchers analyze Google App Engine performance with CPU-intensive applications. \\

CaaS have also been a topic of study,
comparing them to long live containers or performance test among of different provides.
We can find examples of this in \cite{cc} and \cite{dd}.
Comparison between Faas and short-lived containers has also been analyzed
but from a functionality point of view, oversimplificating it and not taking into account the performance \cite{ee}\cite{ff}\cite{gg}. \\

FaaS on its own also has been subject of previous research.
An excellent example is \cite{aa}
where multiple serverless providers are continuously been benchmarked.
Another example of a comparison of Faas providers is \cite{bb}.
In \cite{num14} the researches discuss the advantages of using cloud services
and AWS Lambada for systems that require higher resilence.
Finally, is necessary to mention \cite{laltre},
which focuses in its performance evaluation
and also benchmarking the data transfer to storage and its lifetime of the major cloud functions providers. \\


Although serverless performance has been heavily analyzed,
to the best of our knowledge, their scaling and its consequences to performance
and overhead have not been comprehensively analysed yet, which motivates our research.


\section{Research question}

\begin{itemize}
\item \textbf{How do different platforms compare when with sudden spikes in traffic?}
\end{itemize}

Specifically we will be looking at:

\begin{itemize}

\item \textbf{Does scaling out affect the performance of the services?}
When scaling out to meet demand,
serverless platforms should isolate instances
to provide a consistent level of performance.

\item \textbf{Does scaling out affect platform overhead?}
Everything has a cost,
what we hope to see is that scaling out does not severely degrade the platform's performance,
in terms of instance management.
\end{itemize}


\section{Methods}
\label{methods}

Given the number of services of this type in the current market we defined a selection criteria based on the most popular features \cite{popular1} of serverless computing: dynamically managed runtimes, globally reachable HTTP(S) endpoint, pay only for usage and autoscaling. \\

Having defined these characteristics, we selected the products that provide them offered by the top 5 cloud providers as of 2019 \cite{hh}: Amazon Web Services (AWS), Google Cloud Platform (GCP), Microsoft Azure, Alibaba Cloud, and IBM Cloud. Additionally, we also included Zeit, a startup in the FaaS space popular for its streamlined experience. The products selected can be found in Table \ref{Tab:services}. \\

As mentioned in section \ref{introduction}, the pricing model for PaaS (charge for underlying VMs) does not fit in the 'Pay only for usage' defined in our selection criteria. However GCP App Engine offers the possibility to scale your applications to zero when it is not been used so it fits into our selection criteria. \\

Our choice of workload was an image resizing service implemented in Python, a real world \cite{ii}
use-case and an often used example in what FaaS solutions are good for.
This is a more CPU intensive workload that
does not rely on external services.

We aimed to use the same code (excluding API adapters) for all platforms, running on the highest version of Python 3 available.
We used the datacenters closest to Amsterdam,
with machine types configured with 128MB of memory where available. Based on prior research \cite{laltre}
machine size should not affect platform overhead. \\

Our server code accepts HTTP requests with an image,
resizes it and responds with the resized image.
It also returns the time spent processing the image,
and a unique identifier generated on startup.
Our client code has a 2 phase testing cycle run every hour:
phase 1 (which will be referred as single test) sends 10 requests sequentially,
phase 2 (which will be referred as multi test) starts 50 workers in parallel to each send 10 requests sequentially.
For each request we recorded the total roundtrip time,
in addition to the metadata returned by the server. \\

We aimed to use a stable set of images with similar distributions
of file sizes for both phases.


\begin{table}
%\centering
\begin{adjustwidth}{-0.9cm}{}
 \begin{tabularx}{1.1\textwidth}{p{1cm} X X X X p{3cm}}
 %\hline
 %\rowcolor{Gray}
 \textbf{Type} & \textbf{Platform} & \textbf{Product} & \textbf{Instance Size} & \textbf{Pyhon runtime} & \textbf{Location of the DC} \\
 \hline
 \hline
 FaaS & AWS & Lambda & 128 MB & 3.8 & London, UK \\
 \hline
 FaaS & GCP & Functions & 128 MB & 3.7 & St. Ghislain, BE \\
 \hline
 FaaS & Azure & Functions & 128 MB & 3.7 & NL \\
 \hline
 FaaS & IBM Cloud & Functions & 128 MB & 3.7 & London, UK\\
 \hline
 FaaS & Alibaba Cloud & Function Compute  & 128 MB & 3.6 & Frankfurt, DE\\
 \hline
 FaaS & Zeit & Now  & 128 MB & 3.6 & Brussels, BE\\
 \hline
 CaaS & GCP & Cloud Run & 128 MB & 3.8 & St. Ghislain, BE\\
 \hline
 PaaS & GCP & App Engine & 128 MB & 3.8 & St. Ghislain, BE\\
 \hline

\end{tabularx}
\caption{Serverless services that will be tested in this reaserch.}
\label{Tab:services}
\end{adjustwidth}
\end{table}




\section{Results}

We ran our test over the course of 4 days (including weekdays and weekends). In table \ref{Tab:multi} and table \ref{Tab:single} we can find a summary of the multi and single phases of the test. \\

Looking into the success rate of our requests, we see some transient errors whose effect we consider negligible. However, Alibaba Cloud had a significantly elevated error rate, which we will discuss later. It is also important to mention that we did not observe any variations based on the time of day (see figure \ref{Fig:performance}).

\begin{table}
\centering
 \begin{tabularx}{1\textwidth}{p{4cm} X X X X X X}
 \textbf{Service} & \textbf{Total request} & \textbf{Success rate} & \textbf{Cold starts ratio} & \textbf{Parallel instances} & \textbf{StDev (parallel inst.)} \\
 \hline
 \hline
 AWS Lambda & 47000 & 1 & 0.097 & 50.402 & 3.858\\
 \hline
 GCP Functions & 46972 & 0.999 & 0.062 & 36.348 & 6.231\\
 \hline
 Azure Functions & 46997 & 1 & 0.005 & 3.598 & 0.680\\
 \hline
 IBM Cloud Functions & 47000 & 1 & 0.097 & 50.685 & 5.256\\
 \hline
 Alibaba Cloud Function Compute & 43501 & 0.926 & 0.169 & 81.043 & 4.427\\
 \hline
 Zeit Now & 46995  & 1 & 0.08 & 41.663 & 4.962\\
 \hline
 GCP Cloud Run & 47000 & 1 & 81.163 & 33.575\\
 \hline
 GCP App Engine & 47000 & 1 & 0.049 & 29.402 & 5.985\\
 \hline
\end{tabularx}
\caption{Results multi test.}
\label{Tab:multi}
\end{table}


\begin{table}
\centering
 \begin{tabularx}{1\textwidth}{p{4cm} X X X X X X}
 \textbf{Service} & \textbf{Total request} & \textbf{Success rate} & \textbf{Cold starts ratio} & \textbf{Parallel instances} & \textbf{StDev (parallel inst.)} \\
 \hline
 \hline
 AWS Lambda & 940 & 1 & 0.098 & 1.022 & 0.209\\
 \hline
 GCP Functions & 940 & 1 & 0.027 & 1.022 & 0.209\\
 \hline
 Azure Functions & 940 & 1 & 0.099 & 1.022 & 0.209\\
 \hline
 IBM Cloud Functions & 940 & 1 & 0.099 & 1.022 & 0.209\\
 \hline
 Alibaba Cloud Function Compute & 940 & 1 & 0.098 & 1.022 & 0.209\\
 \hline
 Zeit Now & 940 & 1 & 0.098 & 1.022 & 0.209\\
 \hline
 GCP Cloud Run & 940 & 1 & 0.098 & 1.011 & 0.104\\
 \hline
 GCP App Engine & 940 & 1 & 0.001 & 1.011 & 0.104\\
 \hline

\end{tabularx}
\caption{Results single test.}
\label{Tab:single}
\end{table}

\begin{figure}[h]
\includegraphics[width=14cm]{perfovertime.png}
\centering
\caption{Performance over time of all services tested.}
\label{Fig:performance}
\end{figure}


\subsection{Compute performance}
In Figure \ref{Fig:cputime}
we see the cumulative frequencies for the image processing time
as measured by the server, split by concurrency level (single and multi test phase) and cold/warm starts.


\begin{figure}[h]
\includegraphics[width=14cm]{cputime.png}
\centering
\caption{Acumulative distribution of CPU time.}
\label{Fig:cputime}
\end{figure}


\subsection{Overhead}

In Figure \ref{Fig:overhead} we see the cumulative frequencies for the overhead time
(total roundtrip time - server processing time) 
split by concurrency level(single or multi test phase) and cold/warm starts.
With all else being equal, we attribute the difference to time spent conducting a cold start.


\begin{figure}[h]
\includegraphics[width=14cm]{overhead.png}
\centering
\caption{Acumulative distribution of overhead time.}
\label{Fig:overhead}
\end{figure}


\section{Discussion}
\label{discussion}

\subsection{Scaling}
\label{subscaling}
From Table \ref{Tab:multi}
we observed elevated failure rates for Alibaba Function Compute.
Their documentation \cite{ali}
states they have a limit of 50 instances per service,
and even though our client places a strict limit of 50 inflight requests at any point in time,
their scheduler might not be keeping up
in either reusing an instance or cleaning it up before the next request arrives. 

Also from Table \ref{Tab:multi}
we can see the average number of unique instances per test cycle.
Ideally, this number would be 50,
to match the number of parallel requests we are sending. \\

On the low end,
Azure Functions is notable for only starting 4 instances.
This may be from a documented limitation \cite{azure}
of only starting at most 1 instance per second with HTTP triggers.
Their load balancers still hold the requests in queue,
but they appear to be reluctant to start new instances even as sufficient time has passed. \\

At the high end,
we configured GCP Cloud Run to accept up to 5 concurrent requests per instance,
expecting it to utilise the available memory and CPU more efficiently as this was one of its main selling points.
However, their scheduler also took CPU utilitzation into account,
limiting the usefulness of setting a higher concurrency level with our CPU intensive task.
In hindsight, setting the concurrency level to 1 would have been a fairer comparison. \\

From Figure \ref{Fig:cputime} we can see for AWS, Azure, Zeit, and GCP Functions,
we saw good isolation of workloads,
they performed similarly for warm requests at both concurrency levels.
GCP Cloud Run saw an expected degradation in performance from handling multiple requests,
While Alibaba and IBM were serverely impacted.




\subsection{Warm vs Cold Starts}
An often cited concern of using truly on-demand compute services is the cold start,
when the platforms have to start up a new instance to handle increases in load.
This is such a concern that some some platforms have specific features
built to keep your instances warm, such as AWS Provisioned Concurrency \cite{ams}
and Azure Functions Premium Plan. \\

Our results in Figure \ref{Fig:overhead}
show that, as expected, in most cases,
single concurrency cold starts are slower than warm starts
and the start times are highly consistent.
Cold start times are much more variable at higher concurrency levels,
with the exception of AWS Lambda which appears unaffected.
We suspect this could be due to having the system image on disk,
which is reusable as long as instances are on the same machine,
vs needing to retrieve the image over a network,
but we don't have a solid way of testing this. \\

What is unexpected is in Figure \ref{Fig:cputime},
that that cold starts affect CPU performance.
These instances consistently perform worse on a cold start,
even as they are reused for future requests.
Further testing would be needed to identify the root cause of this.

\subsection{Technologies}
GCP App Engine is the oldest technology being compared.
Under the hood it appears to be an NGINX reverse proxying Python apps through Gunicorn.
Its scaling is controlled by CPU utilization,
and in our case a heavy workload pushes that up
and limits the concurrent requests a single instance can handle.
While it can scale to zero instances,
each instance has a high (15 minute) startup fee \cite{gcp}
resulting in higher costs for short spikes in traffic. \\

The current generation of FaaS are built on a wide variety of technologies.
AWS and GCP have publicly stated that they use their own VMs,
Firecracker and gVisor respectively, to isolate workloads.
The results from Figure \ref{Fig:overhead}
show they provide a highly consistent environment even under load.
Zeit Now uses AWS datacenters \cite{zn}
for their serverless offering.
They behave similarly to AWS Lambda
and we believe that is what they use, but with the largest machine type.
Alibaba Functions, Azure Functions, and IBM Functions
all appear to be implemented on containers,
as either their developer tooling or documentation
hint at the possiblitily of customizing the runtime in not very well documentated ways.
Both Alibaba and IBM exhibit similar characteristics of degraded performance under load,
possibliy from poor isolation between workloads. \\

GCP Cloud Run is a fully managed container runtime implementing the Kubernetes Serving API,
and the only service we tested that exposed the Docker runtime directly.
As expected, single concurrency performance is consistent
but drops when more requests are routed to a single instance.
As the industry moves to converge on Kubernetes as a common interface and runtime,
we expect more fully integrated products in this space in the future.
While other platforms provide container runtimes or hosted Kubernetes,
they are at a lower level in the stack and don't provide the full functionality we were looking for.


\section{Future work}

There are still a lot of possiblities for continiuing the testing of serverless services that we could not cover because of shortage in time. We would like to suggest some further investigations:

\begin{itemize}
\item Test the serverless products with different workloads. This reserch has focused in testing using a CPU bound application but differents types of workloads can be used.
\item Test different concurrency levels. This reserch has demonstarate that perfomance degrades with higher workloads but it would be intresting to investigate on how performance decreases with the increase of different workloads. 
\item Test with diffent image sizes of the serverless services. This reasech has used 128 MB but the providers offer more. Platform overhead should not be affected but the performance of the client's application should \cite{laltre}.
\item Reaserch on other's providers serverless solutions and keep conducting reaserch on all new serverless products as the cloud evolves rapidly and new products are lauch every day.

\end{itemize}



\section{Conclusion}

After this reaserch we can answer our reaserch subquestions and question.

\begin{itemize}

\item \textbf{Does scaling out affect the performance of the services?} AWS Lambda and Zeit Now present no performance deterioration when scaling out. They present lower performance when the request implies a cold start but no differences can be found between a low and a high workload performance.

However Azure, Alibaba Cloud Function Compute, IBM Cloud Function and GCP App Engine present a clear degradation of the performance when the workload is increased and more instances are created. As mentioned in section \ref{subscaling}, comparing GCP Cloud Run would not be a far comparison, but its worth mention that, even its performance with low concurrency is stable, it degradates when scaling out.

\item \textbf{Does scaling out affect platform overhead?} AWS Lambda presents the most stable platform overhead, slightly increasing with cold starts, but mantaint over time. However all other platforms present high variablity in overhead when scaling out specially when multiple instances are created at the same time (multi cold).

\end{itemize}

Finally, our main reaserch quesion:

\begin{itemize}
\item \textbf{How do different platforms compare when with sudden spikes in traffic?}
\end{itemize}
In practice almost all platforms are heavily affected by spikes in traffic. The performance of the uploaded code decreases and its affected by the cold starts needed for iniciating new instances. AWS Lambda is the ony platform tested that has been able to mantain equal performance and overherad in all situations.  \\


We can affirm that, under a CPU bound workload, AWS Lambda has presented the best performance stability(and for extension, instance isolation) and less overhead when increasing the workload in a CPU bound application.


\bibliography{references}
\bibliographystyle{unsrt}
\end{document}
